{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Data Preparation and Predictive Modeling\n",
    "**Example by David Cochran**\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition ...\n",
    "\n",
    "1. Read in the original data.\n",
    "2. Perform preparation steps — rationale explained in a separate EDA notebook.\n",
    "2. Split the data into Train (60%) / Validate (20%) / Test (20%)\n",
    "3. Train, Fit, Test, Evaluate and Compare Models using These Algorithms\n",
    "\n",
    "References and Resources:\n",
    "- [Churn Modeling Notebook by cutterback](https://github.com/cutterback/p03-telco-churn-model/blob/master/Telco-Churn-Classification-Model.ipynb)\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "- https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', font_scale = 1.1)\n",
    "%matplotlib inline\n",
    "from time import time\n",
    "\n",
    "# Machine Learning Training, Scoring, and Metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, roc_auc_score\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Survived      891 non-null    int64  \n",
      " 1   Pclass        891 non-null    int64  \n",
      " 2   Sex           891 non-null    int64  \n",
      " 3   Age           891 non-null    float64\n",
      " 4   Fare          891 non-null    float64\n",
      " 5   Family_count  891 non-null    int64  \n",
      " 6   Cabin_ind     891 non-null    int64  \n",
      "dtypes: float64(2), int64(5)\n",
      "memory usage: 48.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Drop irrelevant columns\n",
    "df.drop(['PassengerId','Name','Ticket'], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing age values with average age\n",
    "df['Age'].fillna(df['Age'].mean(), inplace = True)\n",
    "\n",
    "# Create Family_count from SibSp and Parch\n",
    "df['Family_count'] = df['SibSp'] + df['Parch']\n",
    "\n",
    "# Drop SibSp and Parch\n",
    "df.drop(['SibSp','Parch'], axis=1, inplace=True)\n",
    "\n",
    "# Create Cabin_ind\n",
    "df['Cabin_ind'] = np.where(df['Cabin'].isnull(), 0, 1)\n",
    "\n",
    "# Drop Cabin and Embarked\n",
    "df.drop(['Cabin','Embarked'], axis=1, inplace=True)\n",
    "\n",
    "# Convert sex to numeric indicator\n",
    "gender_map = {'male': 0, 'female': 1}\n",
    "df['Sex'] = df['Sex'].map(gender_map)\n",
    "\n",
    "# View Updated Dataframe Info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Family_count</th>\n",
       "      <th>Cabin_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age     Fare  Family_count  Cabin_ind\n",
       "0         0       3    0  22.0   7.2500             1          0\n",
       "1         1       1    1  38.0  71.2833             1          1\n",
       "2         1       3    1  26.0   7.9250             0          0\n",
       "3         1       1    1  35.0  53.1000             1          1\n",
       "4         0       3    0  35.0   8.0500             0          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first records\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Train, Validation, and Test Sets\n",
    "See the [train_test_split docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "- Train = 60%\n",
    "- Validation = 20%\n",
    "- Test = 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features, Labels:\n",
      "534 Records, 534 Labels\n",
      "-----------\n",
      "Validation Features, Labels:\n",
      "179 Records, 179 Labels\n",
      "-------\n",
      "Test Features, Labels:\n",
      "178 Records, 178 Labels\n"
     ]
    }
   ],
   "source": [
    "features = df.drop('Survived', axis=1)\n",
    "labels = df['Survived']\n",
    "# First Split: Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "# Second Split: Test into Validation/Test\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Report the number of records and labels in each new split:\n",
    "print('Training Features, Labels:')\n",
    "print(f'{X_train.shape[0]} Records, {len(y_train)} Labels')\n",
    "print('-----------')\n",
    "print('Validation Features, Labels:')\n",
    "print(f'{X_val.shape[0]} Records, {len(y_val)} Labels')\n",
    "print('-------')\n",
    "print('Test Features, Labels:')\n",
    "print(f'{X_test.shape[0]} Records, {len(y_test)} Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Variable Names for Machine Learning\n",
    "# Training Set (60%)\n",
    "tr_features = X_train\n",
    "tr_labels = y_train\n",
    "\n",
    "# Validation Set (20%)\n",
    "val_features = X_val\n",
    "val_labels = y_val\n",
    "\n",
    "# Test Set (20%)\n",
    "test_features = X_test\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Tune Models using 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [GridSearchCV docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "## Setup\n",
    "- Create an empty list to hold our best models\n",
    "- Set fundamental parameters\n",
    "- Define functions to implement gridsearch and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold our best models\n",
    "models = []\n",
    "\n",
    "# Establish number of splits for k-fold cross-validation\n",
    "k = 5\n",
    "\n",
    "# Specify random seed value\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instiantiate_grid\n",
    "- Set up defaults for Gridsearch CV, incluing the scorers we want, and the priority\n",
    "- Set to default to prioritize roc_auc in model fitting and selecting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up defaults for Gridsearch CV, incluing the scorers we want, and the priority\n",
    "# Set to default to prioritize roc_auc\n",
    "def instantiate_grid(algorithm, param_grid, refit='roc_auc'):\n",
    "        \n",
    "    # Set up scorers\n",
    "    scoring = {\n",
    "                'roc_auc': make_scorer(roc_auc_score, greater_is_better=True,\n",
    "                         needs_threshold=False),\n",
    "                'f1': make_scorer(f1_score),\n",
    "                'accuracy': make_scorer(accuracy_score),\n",
    "                'precision': make_scorer(precision_score),\n",
    "                'recall': make_scorer(recall_score)\n",
    "              }\n",
    "\n",
    "    cv = GridSearchCV(\n",
    "                        estimator=algorithm,\n",
    "                        param_grid=param_grid, \n",
    "                        scoring=scoring,\n",
    "                        refit=refit, \n",
    "                        cv=StratifiedKFold(n_splits=k, random_state=seed, shuffle=True)\n",
    "                     )\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show_grid_metrics\n",
    "- Specify metrics we desire from GridsearchCV results\n",
    "- Sort by desired metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display grid results ranked\n",
    "def show_grid_metrics(cv_results, sort_by, top_n=10):\n",
    "\n",
    "    # Specify evaluation metrics that we desire from GridSearchCV results\n",
    "    metrics = ['params',\n",
    "               'rank_test_roc_auc',\n",
    "               'mean_test_roc_auc',\n",
    "               'rank_test_f1',\n",
    "               'mean_test_f1',\n",
    "               'mean_test_accuracy', \n",
    "               'mean_test_precision',\n",
    "               'mean_test_recall'\n",
    "              ]\n",
    "    \n",
    "    cv_results_metrics = cv_results.loc[:, metrics]\n",
    "    cv_results_metrics.sort_values(by=[sort_by], ascending=False, inplace=True)\n",
    "\n",
    "    return cv_results_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: Train and Tune \n",
    "- [GridSearchCV Docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- [LogisticRegression Docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP-PERFORMING LR MODELS\n",
      "\n",
      "Training Latency: 2.16s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.780835</td>\n",
       "      <td>1</td>\n",
       "      <td>0.728283</td>\n",
       "      <td>0.797973</td>\n",
       "      <td>0.751291</td>\n",
       "      <td>0.707374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 1000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.780835</td>\n",
       "      <td>1</td>\n",
       "      <td>0.728283</td>\n",
       "      <td>0.797973</td>\n",
       "      <td>0.751291</td>\n",
       "      <td>0.707374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.779385</td>\n",
       "      <td>3</td>\n",
       "      <td>0.726369</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.704476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.775912</td>\n",
       "      <td>4</td>\n",
       "      <td>0.721795</td>\n",
       "      <td>0.794608</td>\n",
       "      <td>0.750555</td>\n",
       "      <td>0.695695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.652323</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493244</td>\n",
       "      <td>0.721650</td>\n",
       "      <td>0.815924</td>\n",
       "      <td>0.353836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 0.001}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.613164</td>\n",
       "      <td>6</td>\n",
       "      <td>0.413579</td>\n",
       "      <td>0.689109</td>\n",
       "      <td>0.748943</td>\n",
       "      <td>0.286445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params  rank_test_roc_auc  mean_test_roc_auc  rank_test_f1  \\\n",
       "4    {'C': 100}                  1           0.780835             1   \n",
       "5   {'C': 1000}                  1           0.780835             1   \n",
       "3     {'C': 10}                  3           0.779385             3   \n",
       "2      {'C': 1}                  4           0.775912             4   \n",
       "1   {'C': 0.01}                  5           0.652323             5   \n",
       "0  {'C': 0.001}                  6           0.613164             6   \n",
       "\n",
       "   mean_test_f1  mean_test_accuracy  mean_test_precision  mean_test_recall  \n",
       "4      0.728283            0.797973             0.751291          0.707374  \n",
       "5      0.728283            0.797973             0.751291          0.707374  \n",
       "3      0.726369            0.796855             0.750547          0.704476  \n",
       "2      0.721795            0.794608             0.750555          0.695695  \n",
       "1      0.493244            0.721650             0.815924          0.353836  \n",
       "0      0.413579            0.689109             0.748943          0.286445  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute GridSearch and Report Results\n",
    "\n",
    "# Specify Algorithm shortname\n",
    "name = 'LR'\n",
    "\n",
    "# Specify algorithm with desired default parameters\n",
    "algorithm = LogisticRegression(random_state=seed, fit_intercept=False, max_iter=500, n_jobs=-1)\n",
    "\n",
    "# Set parameters for GridSearch, to identify best parameters\n",
    "param_grid = {\n",
    "    'C': [.001, .01, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# /////////////////////////////////////////////////////////\n",
    "# Standard process for cross-validating using Gridsearch\n",
    "# /////////////////////////////////////////////////////////\n",
    "\n",
    "# Instantiate gridsearch object for this algorithm\n",
    "cv = instantiate_grid(algorithm, param_grid)\n",
    "\n",
    "start = time()\n",
    "\n",
    "# Activate gridsearch\n",
    "cv.fit(features, labels)\n",
    "\n",
    "end = time()\n",
    "latency = round((end-start), 2)\n",
    "\n",
    "# Create dataframe from gridsearch cv_results_\n",
    "cv_results = pd.DataFrame.from_dict(cv.cv_results_)\n",
    "\n",
    "# Print heading\n",
    "print(f'\\nTOP-PERFORMING {name} MODELS\\n')\n",
    "print(f'Training Latency: {latency}s')\n",
    "\n",
    "# Display the results for top 5 sorted by ROC-AUC\n",
    "show_grid_metrics(cv_results, 'mean_test_roc_auc', top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7808347500439056"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best score is the best mean_test_roc_auc score, as configured in GridSearchCV setup\n",
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, fit_intercept=False, max_iter=500, n_jobs=-1,\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best estimator is the model with the best mean_test_roc_auc score\n",
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best params are the params for the best estimator\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, use this to view the entire GridSearch CV results object\n",
    "# gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the best model to our models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR MODEL {'C': 100}\n",
      "\t AUC Score: 78.083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Provide desired model shortname\n",
    "name = 'LR'\n",
    "\n",
    "# Auto-add model information to models list\n",
    "model = cv.best_estimator_\n",
    "params = cv.best_params_\n",
    "score = round(cv.best_score_ * 100, 3)\n",
    "models.append({'Name': name, 'Params': params, 'Model': model, 'Score': score})\n",
    "for m in models:\n",
    "    print(f'{m[\"Name\"]} MODEL {m[\"Params\"]}\\n\\t AUC Score: {m[\"Score\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ///////////////////////////////////////////////\n",
    "# Unfinished below\n",
    "# ///////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_score = cv.best_estimator_.decision_function(X_train)\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "disp = plot_precision_recall_curve(cv.best_estimator_, X_train, y_train)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting AUC with GridsearchCV\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "# cv.cv_results_['mean_test_score']\n",
    "cv.cv_results_['_recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the best model and its metrics to our models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide desired model shortname\n",
    "name = 'LR'\n",
    "\n",
    "# Auto-add model information to models list\n",
    "model = cv.best_estimator_\n",
    "params = cv.best_params_\n",
    "accuracy = round(cv.best_score_ * 100, 3)\n",
    "\n",
    "models.append({\n",
    "    'Name': name, \n",
    "    'Params': params, \n",
    "    'Model': model, \n",
    "    'Accuracy': accuracy\n",
    "})\n",
    "\n",
    "for m in models:\n",
    "    print(f'{m[\"Name\"]} MODEL {m[\"Params\"]}')\n",
    "    print(f'\\t Training Accuracy: {m[\"Accuracy\"]}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    cm = confusion_matrix(y_train, model['Model'].predict(X_train))\n",
    "    print(model['Name'])\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Confusion Matrix\n",
    "- [Scikit-Learn Confusion Matrix Docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- [Scikit-Learn Model Evaluation Docs](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Confusion Matrix Visualization - Dennis T](https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea)\n",
    "- [Stackoverflow example](https://stackoverflow.com/a/36165952)\n",
    "- [Various Confusion Matrix Plots - Kaggle Notebook](https://www.kaggle.com/agungor2/various-confusion-matrix-plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the basic confusion matrix\n",
    "y_predict = cv.best_estimator_.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn heatmap with labels formatted as whole integers and annotation fontsize adjusted\n",
    "cmdf = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (6,4))\n",
    "sns.heatmap(cmdf, annot=True, fmt = '.0f', cmap='Blues', annot_kws={\"size\": 14});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Seaborn heatmap with data calculated as percentages\n",
    "# # with labels formatted as percentages and annotation fontsize adjusted\n",
    "# cmdf = pd.DataFrame(cm)\n",
    "# plt.figure(figsize = (6,4))\n",
    "# sns.heatmap(cmdf / np.sum(cmdf), annot=True, fmt = '.2%', cmap='Blues', annot_kws={\"size\": 14});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix displaying labels, percentages, and counts\n",
    "# See: https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cm.flatten()]\n",
    "labels = [f'{v1}\\n\\n{v2}\\n\\n{v3}' for v1, v2, v3 in\n",
    "          zip(group_names,group_percentages,group_counts)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', annot_kws={\"size\": 14}); # NOTE: fmt='' is required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to display confusion matrix\n",
    "# Confusion matrix displaying labels, percentages, and counts\n",
    "# See: https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "\n",
    "def cm_plot(cm):\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    labels = [f'{v1}\\n\\n{v2}\\n\\n{v3}' for v1, v2, v3 in\n",
    "              zip(group_names,group_percentages,group_counts)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize = (7,5))\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', annot_kws={\"size\": 14}); # NOTE: fmt='' is required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: Train and Tune \n",
    "- [RandomForestClassifier Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [GridSearchCV Docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify algorithm\n",
    "algorithm = RandomForestClassifier()\n",
    "\n",
    "# Set parameters for GridSearch, to identify best parameters\n",
    "parameters = {\n",
    "    'max_depth': [2, 4, 8, 16, 32, None],\n",
    "    'n_estimators': [5, 50, 250]\n",
    "}\n",
    "\n",
    "# Run the classifier using k-fold cross-validation with Gridsearch to identify best parameters\n",
    "start = time()\n",
    "cv = GridSearchCV(algorithm, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels)\n",
    "end = time()\n",
    "latency = round((end - start), 2)\n",
    "\n",
    "print_results(cv, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the best model to our models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide desired model shortname\n",
    "name = 'RF'\n",
    "\n",
    "# Auto-add model information to models list\n",
    "model = cv.best_estimator_\n",
    "params = cv.best_params_\n",
    "score = round(cv.best_score_ * 100, 3)\n",
    "models.append({'Name': name, 'Params': params, 'Model': model, 'Score': score})\n",
    "for m in models:\n",
    "    print(f'{m[\"Name\"]} MODEL {m[\"Params\"]}\\n\\t Score: {m[\"Score\"]} accuracy with training data\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron: Train and Tune\n",
    "\n",
    "[MLPClassifier Docs](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify algorithm\n",
    "algorithm = MLPClassifier()\n",
    "\n",
    "# Set parameters for GridSearch, to identify best parameters\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'max_iter':[10000] # Raise from default 200 to avoid max iteration warnings\n",
    "}\n",
    "\n",
    "# Run the classifier using k-fold cross-validation with Gridsearch to identify best parameters\n",
    "start = time()\n",
    "cv = GridSearchCV(algorithm, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels)\n",
    "end = time()\n",
    "latency = round((end - start), 2)\n",
    "\n",
    "print_results(cv, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the best model to our models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide desired model shortname\n",
    "name = 'MLP'\n",
    "\n",
    "# Auto-add model information to models list\n",
    "model = cv.best_estimator_\n",
    "params = cv.best_params_\n",
    "score = round(cv.best_score_ * 100, 3)\n",
    "models.append({'Name': name, 'Params': params, 'Model': model, 'Score': score})\n",
    "for m in models:\n",
    "    print(f'{m[\"Name\"]} MODEL {m[\"Params\"]}\\n\\t Score: {m[\"Score\"]} accuracy with training data\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting: Train and Tune\n",
    "\n",
    "[GradientBoostingClassifier Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Specify the algorithm to use\n",
    "algorithm = GradientBoostingClassifier()\n",
    "\n",
    "# Set parameters for GridSearch, to identify best parameters\n",
    "parameters = {\n",
    "    'max_depth': [5, 50, 250, 500],\n",
    "    'n_estimators': [1, 3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Run the classifier using k-fold cross-validation with Gridsearch to identify best parameters\n",
    "start = time()\n",
    "cv = GridSearchCV(algorithm, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels)\n",
    "end = time()\n",
    "latency = round((end - start), 2)\n",
    "\n",
    "print_results(cv, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the best model to our models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide desired model shortname\n",
    "name = 'GB'\n",
    "\n",
    "# Auto-add model information to models list\n",
    "model = cv.best_estimator_\n",
    "params = cv.best_params_\n",
    "score = round(cv.best_score_ * 100, 3)\n",
    "models.append({'Name': name, 'Params': params, 'Model': model, 'Score': score})\n",
    "for m in models:\n",
    "    print(f'{m[\"Name\"]} MODEL {m[\"Params\"]}\\n\\t Score: {m[\"Score\"]} accuracy with training data\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the full models list\n",
    "The models list contains a Python dictionary for each model, with Name, Params, Score (with training data), _and the full Model object_. The model object carries all needed model information with it (not always fully displayed, but available in its attributes) and is what will be used in Validation and Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the full models list\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "We'll evaluate the performance of our three best models with the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for model evaluation\n",
    "def model_eval(model, features, labels):\n",
    "    start = time()\n",
    "    pred = model['Model'].predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred)*100, 3)\n",
    "    precision = round(precision_score(labels, pred)*100, 3)\n",
    "    recall = round(recall_score(labels, pred)*100, 3)\n",
    "    latency = round((end - start), 5)\n",
    "    print(f'{model[\"Name\"]} MODEL: {model[\"Params\"]}')\n",
    "    print(f'    Score: Accuracy {accuracy} | Precision {precision} | Recall {recall} | Latency {latency}s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of each top model with our validation data set\n",
    "for model in models:\n",
    "    model_eval(model, val_features, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "As this is a learning exercise, we'll now evaluate all three of our top models on the test set as well, to see how their performance differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of each top model with our test data set\n",
    "for model in models:\n",
    "    model_eval(model, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
